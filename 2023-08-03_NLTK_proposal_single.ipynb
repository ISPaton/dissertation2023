{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b422c99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\my37env\\lib\\site-packages\\geopandas\\_compat.py:115: UserWarning: The Shapely GEOS version (3.10.1-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n",
      "C:\\ProgramData\\Anaconda3\\envs\\my37env\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n",
      "C:\\ProgramData\\Anaconda3\\envs\\my37env\\lib\\site-packages\\seaborn\\rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "C:\\ProgramData\\Anaconda3\\envs\\my37env\\lib\\site-packages\\setuptools\\_distutils\\version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapefile\n",
    "import logging\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import re\n",
    "import pyLDAvis.gensim_models\n",
    "import numpy as np\n",
    "import matplotlib.font_manager as fm\n",
    "import gensim\n",
    "import seaborn as sns\n",
    "from os import path\n",
    "from gensim.models import LdaMulticore\n",
    "from PIL import Image\n",
    "from IPython.display import Markdown, display\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk import pos_tag\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "from collections import Counter\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b5e264",
   "metadata": {},
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eb029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "def text_preprocessing(text):\n",
    "    if text is None:\n",
    "        return []  # Return an empty list if text is None\n",
    "   \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text) \n",
    "\n",
    "    # Tokenize each word\n",
    "    text = nltk.WordPunctTokenizer().tokenize(text)\n",
    "\n",
    "    # Lemmatize each word - Can be disabled using hash\n",
    "    #text = [nltk.stem.WordNetLemmatizer().lemmatize(token, pos='v') for token in text if len(token) > 1]\n",
    "    #text = [nltk.stem.WordNetLemmatizer().lemmatize(token, pos='n') for token in text if len(token) > 1]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    return text\n",
    "\n",
    "# Convert list to string\n",
    "def to_string(text):\n",
    "    text = ' '.join(map(str, text))\n",
    "    return text\n",
    "\n",
    "# clean text\n",
    "def clean_text(text, exceptions=[]):\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize the text and filter out short words (length <= n)\n",
    "    words = text.split()\n",
    "    cleaned_words = [word for word in words if len(word) > 3 or word in exceptions]\n",
    "    \n",
    "    # Join the cleaned words back into a string\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    return cleaned_text\n",
    "\n",
    "#convert words to numbers\n",
    "def words_to_numbers(text):\n",
    "    word_to_number = {\n",
    "        'one': '1',\n",
    "        'two': '2',\n",
    "        'three': '3',\n",
    "        'four': '4',\n",
    "        'five': '5',\n",
    "        'six': '6',\n",
    "        'seven': '7',\n",
    "        'eight': '8',\n",
    "        'nine': '9',\n",
    "        'ten': '10',\n",
    "        'single': '1'  \n",
    "    }\n",
    "\n",
    "    def replace_word(match):\n",
    "        word = match.group(0).lower()\n",
    "        return word_to_number.get(word, '')\n",
    "\n",
    "    numeric_values = re.findall(r'\\b(?:one|two|three|four|five|six|seven|eight|nine|ten)\\b', str(text), flags=re.IGNORECASE)\n",
    "    extracted_values = [word_to_number[word.lower()] for word in numeric_values] if numeric_values else None\n",
    "    cleaned_text = re.sub(r'\\b(?:one|two|three|four|five|six|seven|eight|nine|ten|single)\\b', replace_word, str(text), flags=re.IGNORECASE)\n",
    "    return (cleaned_text, extracted_values)\n",
    "\n",
    "# extract specified words - use class\n",
    "def extract_classes(text):\n",
    "    # Pattern to match specific words with word breaks at the end and possible word breaks at the start\n",
    "    pattern = r'\\b(?:class\\s\\d+|classes(?:\\s\\d+,)+\\d+)\\b'\n",
    "    \n",
    "    # Find all the matched patterns in the text\n",
    "    specific_words = re.findall(pattern, str(text), flags=re.IGNORECASE)\n",
    "    \n",
    "    # Join the matched patterns into a single string, separated by commas\n",
    "    specific_words_str = ','.join(specific_words) if specific_words else None\n",
    "    return specific_words_str\n",
    "\n",
    "# remove specified words - use class\n",
    "def remove_classes(text):\n",
    "    # Pattern to match specific words with word breaks at the end and possible word breaks at the start\n",
    "    pattern = r'\\b(?:class\\s\\d+|classes(?:\\s\\d+,)+\\d+)\\b'\n",
    "    \n",
    "    # Replace the matched patterns with an empty string\n",
    "    cleaned_text = re.sub(pattern, '', str(text), flags=re.IGNORECASE)\n",
    "    return cleaned_text\n",
    "\n",
    "# extract and remove numeric with copy to new column\n",
    "def extract_and_remove_numeric(text):\n",
    "    def replace_numeric(match):\n",
    "        return ''\n",
    "    \n",
    "    # Pattern to match numbers with word breaks at the start, end, or both\n",
    "    pattern = r'\\b\\d+(?:\\.\\d+)?\\b'\n",
    "    \n",
    "    # Find numeric values\n",
    "    numeric_values = re.findall(pattern, str(text))\n",
    "    extracted_values = ','.join(numeric_values) if numeric_values else None\n",
    "    \n",
    "    # Replace standalone numeric values with an empty string\n",
    "    cleaned_text = re.sub(r'\\b(?:{})\\b'.format('|'.join(numeric_values)), replace_numeric, str(text))\n",
    "    return (cleaned_text, extracted_values)\n",
    "\n",
    "def remove_standalone_numeric(words_list):\n",
    "    pattern = r'\\b\\d+(?:\\.\\d+)?\\b'\n",
    "    return [word for word in words_list if not re.match(pattern, word)]\n",
    "\n",
    "# extract specific words to new column\n",
    "def extract_units(text):\n",
    "    # Pattern to match specific words with word breaks at the end and possible word breaks at the start\n",
    "    pattern = r'\\b(?:hectares|ha|units|dwellinghouse|dwellinghouses|dwellings|metres|m|storey)\\b'\n",
    "    specific_words = re.findall(pattern, str(text), flags=re.IGNORECASE)\n",
    "    return ','.join(specific_words) if specific_words else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e7f3f6",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff6dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# add additional optional stop words to the set\n",
    "optional_stop_words = {'and','all','by','for','more','none','not','null','of','or','over','than','with','local','major','class','storey'}\n",
    "stop_words.update(optional_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be501d1a",
   "metadata": {},
   "source": [
    "### Read in data for Planning Applications from local directory with subset options and drop year 2020 and 2021 due to covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1555ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in planning app data\n",
    "planapp_gdf_read = gpd.read_file('pub_plnapppol.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0f7f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "planapp_gdf = planapp_gdf_read[['year', 'local_auth', 'proposal','dev_desc','appl_desc','stat_desc', 'xcoord','ycoord']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove 2020 and 2021 covid years\n",
    "planapp_gdf = planapp_gdf[planapp_gdf['year'] != 2020] \n",
    "planapp_gdf = planapp_gdf[planapp_gdf['year'] != 2021]\n",
    "planapp_gdf = planapp_gdf[planapp_gdf['year'] != 3016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b357b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random selection or selection by year or authority\n",
    "subset_size = 100000\n",
    "# Define the range of years you want to select\n",
    "start_year = 2017\n",
    "end_year = 2019\n",
    "#planapp_gdf_year = planapp_gdf[planapp_gdf['year'].isin(range(start_year, end_year+1))]\n",
    "planapp_gdf_auth = planapp_gdf[planapp_gdf['local_auth'] == 'Glasgow City']\n",
    "#planapp_gdf = planapp_gdf_read.sample(n=subset_size)\n",
    "planapp_gdf = planapp_gdf_auth\n",
    "#planapp_gdf = planapp_gdf_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b38b25",
   "metadata": {},
   "source": [
    "### Define columns for Description, Application, Status, Proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f1a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create columns for later handling\n",
    "planapp_gdf[\"text_proposal\"] = planapp_gdf[\"proposal\"].str.lower()\n",
    "planapp_gdf[\"text_desc\"] = planapp_gdf[\"dev_desc\"].str.lower()\n",
    "planapp_gdf[\"text_app\"] = planapp_gdf[\"appl_desc\"].str.lower()\n",
    "planapp_gdf[\"text_status\"] = planapp_gdf[\"stat_desc\"].str.lower()\n",
    "planapp_gdf[\"feature\"] = planapp_gdf[\"text_proposal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fdca04",
   "metadata": {},
   "source": [
    "### Words to numbers and remove numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ecd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change words to numbers\n",
    "planapp_gdf[['feature_cleaned_1', 'numeric_values']] = planapp_gdf['feature'].apply(words_to_numbers).apply(pd.Series)\n",
    "# Extract use classes\n",
    "planapp_gdf['use_class'] = planapp_gdf['feature_cleaned_1'].apply(extract_classes)\n",
    "#planapp_gdf['feature_cleaned_3'] = planapp_gdf['feature_cleaned_2'].apply(remove_specific_words_with_word_break)\n",
    "# Save the DataFrame with the original text and the extracted specific words removed to CSV\n",
    "planapp_gdf['use_class'].to_csv('useclass.csv', index=False)\n",
    "# Apply the function to the \"text_proposal\" column to extract numeric values and remove them\n",
    "planapp_gdf[['feature_cleaned_2', 'quantity']] = planapp_gdf['feature_cleaned_1'].apply(extract_and_remove_numeric).apply(pd.Series)\n",
    "planapp_gdf['feature_cleaned_2'] = planapp_gdf['feature_cleaned_2'].apply(lambda words_list: remove_standalone_numeric(words_list))\n",
    "planapp_gdf['feature_cleaned_2'] = planapp_gdf['feature_cleaned_2'].apply(lambda words_list: ''.join(words_list))\n",
    "#extract and remove units\n",
    "planapp_gdf['units'] = planapp_gdf['feature'].apply(extract_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a351464",
   "metadata": {},
   "source": [
    "### Pull out list of smallest words to identify additional stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b8c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NLTK stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Filter out None values in the 'text_desc' column\n",
    "desc_words = ' '.join(list(planapp_gdf['text_desc'].dropna().values))\n",
    "\n",
    "# Count and find the n most frequent words with 4 characters or less\n",
    "word_counter = Counter(desc_words.split())\n",
    "most_frequent = [word for word, count in word_counter.most_common(30) if len(word) <= 4]\n",
    "\n",
    "# Filter the words with 4 characters or less and not present in NLTK stopwords\n",
    "filtered_most_frequent = [word for word in most_frequent if word not in nltk_stopwords]\n",
    "\n",
    "# Filter the words with 4 characters or less from the original word_counter\n",
    "word_counter_filtered = {word: count for word, count in word_counter.items() if len(word) <= 4}\n",
    "\n",
    "# Convert the filtered word_counter dictionary to a DataFrame\n",
    "df_filtered = pd.DataFrame.from_dict(word_counter_filtered, orient='index', columns=[\"count\"])\n",
    "df_filtered.index.name = \"words\"\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df_filtered.to_csv('word_counter_smallest_proposal.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a7fc3",
   "metadata": {},
   "source": [
    "### Pre processing and visualisation via word count and word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f66f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "planapp_gdf[\"feature_cleaned_3\"] = list(map(text_preprocessing, planapp_gdf.feature_cleaned_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b664f1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of lists into a single list of words\n",
    "desc_words_list = [word for sublist in planapp_gdf['feature_cleaned_3'] for word in sublist]\n",
    "\n",
    "# Join all word corpus\n",
    "desc_words = ' '.join(desc_words_list)\n",
    "\n",
    "# Count and find the n most frequent after cleaning\n",
    "word_counter = Counter(desc_words.split())\n",
    "most_frequent = word_counter.most_common(20)\n",
    "\n",
    "# Bar plot of frequent words\n",
    "fig = plt.figure(1, figsize=(20, 10))\n",
    "_ = pd.DataFrame(most_frequent, columns=(\"words\", \"count\"))\n",
    "sns.barplot(x='words', y='count', data=_, palette='winter')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c99c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all words from the 'feature_cleaned_3' column into a single list\n",
    "desc_words_list = [word for sublist in planapp_gdf['feature_cleaned_3'] for word in sublist]\n",
    "\n",
    "# Join all words into a single string\n",
    "data = ' '.join(desc_words_list)\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_frequencies = {word: desc_words_list.count(word) for word in set(desc_words_list)}\n",
    "\n",
    "# Generate the word cloud using word frequencies\n",
    "wordcloud = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(word_frequencies)\n",
    "\n",
    "# Display the generated image\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6af43",
   "metadata": {},
   "source": [
    "### Tokenisation of phrases to create words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a635dd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = gensim.corpora.Dictionary(planapp_gdf[\"feature_cleaned_3\"])\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in planapp_gdf[\"feature_cleaned_3\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf33e943",
   "metadata": {},
   "source": [
    "### First model run to calculate coherence and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6904ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the function to calculate perplexity\n",
    "def calculate_perplexity(lda_model, corpus, dictionary):\n",
    "    return lda_model.log_perplexity(corpus), lda_model.bound(corpus)\n",
    "\n",
    "# Define alpha and beta\n",
    "alpha = 0.5\n",
    "beta = 0.1  \n",
    "\n",
    "# Set up the number of topics to iterate over\n",
    "num_topics_range = range(1, 31)  # Choose the range of the number of topics to evaluate\n",
    "\n",
    "# Create lists to store the results\n",
    "number_of_topics = []\n",
    "coherence_scores = []\n",
    "perplexity_scores = []\n",
    "\n",
    "# Train LDA models with different numbers of topics and calculate perplexity and coherence\n",
    "for num_topics in num_topics_range:\n",
    "    lda_model = LdaMulticore(corpus=corpus,\n",
    "                             id2word=id2word,\n",
    "                             iterations=50,\n",
    "                             num_topics=num_topics,\n",
    "                             alpha=alpha,\n",
    "                             eta=beta,  # 'eta' is used for beta\n",
    "                             workers=15,\n",
    "                             passes=10)\n",
    "\n",
    "    perplexity, bound = calculate_perplexity(lda_model, corpus, id2word)\n",
    "    perplexity_scores.append(perplexity)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                         texts=planapp_gdf[\"feature_cleaned_3\"],\n",
    "                                         dictionary=id2word,\n",
    "                                         coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    number_of_topics.append(num_topics)\n",
    "    coherence_scores.append(coherence_lda)\n",
    "\n",
    "    # Print coherence score and perplexity for each iteration\n",
    "    print(f\"Iteration {num_topics}: Coherence Score = {coherence_lda}, Perplexity = {perplexity}, Bound = {bound}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_metrics = pd.DataFrame({'number_of_topics': number_of_topics,\n",
    "                              'coherence_score': coherence_scores,\n",
    "                              'perplexity_score': perplexity_scores})\n",
    "\n",
    "# Compute moving average with window size 3 (you can adjust the window size as needed)\n",
    "topic_metrics['coherence_score_smoothed'] = topic_metrics['coherence_score'].rolling(window=3, min_periods=1).mean()\n",
    "topic_metrics['perplexity_score_smoothed'] = topic_metrics['perplexity_score'].rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "# Plot the smoothed coherence scores and perplexity scores on the same plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=topic_metrics, x='number_of_topics', y='coherence_score_smoothed', label='Coherence Score')\n",
    "#sns.lineplot(data=topic_metrics, x='number_of_topics', y='perplexity_score', label='Perplexity Score')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Coherence vs. Number of Topics')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebca9267",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "#sns.lineplot(data=topic_metrics, x='number_of_topics', y='coherence_score_smoothed', label='Coherence Score')\n",
    "sns.lineplot(data=topic_metrics, x='number_of_topics', y='perplexity_score_smoothed', label='Perplexity Score')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Perplexity vs. Number of Topics')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b0bf2e",
   "metadata": {},
   "source": [
    "### Second model run with selected number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd09353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of topics \n",
    "n_topics = 17\n",
    "\n",
    "# Run the LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=n_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           alpha=alpha,\n",
    "                                            eta=beta,                                            \n",
    "                                           iterations=50,\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3986e5b",
   "metadata": {},
   "source": [
    "### Visualisation using pyLDAvis and save to HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374f413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Prepare the visualization data\n",
    "vis_data = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "\n",
    "# Convert topic_info to a DataFrame\n",
    "topic_info_df = pd.DataFrame(vis_data.topic_info)\n",
    "\n",
    "# Convert token_table to a DataFrame\n",
    "token_table_df = pd.DataFrame(vis_data.token_table)\n",
    "\n",
    "# Convert topic_coordinates to a DataFrame and replace complex numbers with real values\n",
    "topic_coords_df = pd.DataFrame(vis_data.topic_coordinates.applymap(np.real))\n",
    "\n",
    "# Convert any remaining NaN values to 0\n",
    "topic_coords_df = topic_coords_df.fillna(0)\n",
    "\n",
    "# Create a new PreparedData object with the updated data\n",
    "updated_data = pyLDAvis.PreparedData(topic_coordinates=topic_coords_df,\n",
    "                                    topic_info=topic_info_df,\n",
    "                                    token_table=token_table_df,\n",
    "                                    R=vis_data.R,\n",
    "                                    lambda_step=vis_data.lambda_step,\n",
    "                                    plot_opts=vis_data.plot_opts,\n",
    "                                    topic_order=vis_data.topic_order)\n",
    "\n",
    "# Display the visualization\n",
    "pyLDAvis.display(updated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad42c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis_data, 'lda_visualization_proposal_single3aug.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43fb15b",
   "metadata": {},
   "source": [
    "### Review of use classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "planapp_gdf.to_csv('planapp_gdf', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad52048",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_class_filtered = planapp_gdf[\"use_class\"][planapp_gdf[\"use_class\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ','.join(use_class_filtered)\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_frequencies = {word: data.count(word) for word in set(data.split(','))}\n",
    "\n",
    "# Generate the word cloud using word frequencies\n",
    "wordcloud = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(word_frequencies)\n",
    "\n",
    "# Display the generated image\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377e26dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the comma-separated values to individual classes\n",
    "classes_list = [class_name.strip() for row in data for class_str in row.split(',') for class_name in class_str.split()]\n",
    "\n",
    "# Calculate frequencies of classes in the corpus\n",
    "class_frequencies = {}\n",
    "for class_name in classes_list:\n",
    "    class_frequencies[class_name] = class_frequencies.get(class_name, 0) + 1\n",
    "\n",
    "class_wordcloud = WordCloud(background_color='white', width=1000, height=600).generate_from_frequencies(class_frequencies)\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Set the figure size for the plot\n",
    "plt.imshow(class_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Classes Word Cloud', fontsize=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
