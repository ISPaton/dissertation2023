{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b422c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from IPython.display import Markdown, display\n",
    "import shapefile\n",
    "import logging\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.font_manager as fm\n",
    "import gensim\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import re\n",
    "import pyLDAvis.gensim_models\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk import pos_tag\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel, LdaModel, LdaMulticore\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from collections import Counter\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b5e264",
   "metadata": {},
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eb029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "def text_preprocessing(text):\n",
    "    if text is None:\n",
    "        return []  # Return an empty list if text is None\n",
    "   \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text) \n",
    "\n",
    "    # Tokenize each word\n",
    "    text = nltk.WordPunctTokenizer().tokenize(text)\n",
    "\n",
    "    # Lemmatize each word\n",
    "    ##text = [nltk.stem.WordNetLemmatizer().lemmatize(token, pos='v') for token in text if len(token) > 1]\n",
    "    text = [nltk.stem.WordNetLemmatizer().lemmatize(token, pos='n') for token in text if len(token) > 1]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    return text\n",
    "\n",
    "# Convert list to string\n",
    "def to_string(text):\n",
    "    text = ' '.join(map(str, text))\n",
    "    return text\n",
    "\n",
    "# clean text\n",
    "def clean_text(text, exceptions=[]):\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize the text and filter out short words (length <= n)\n",
    "    words = text.split()\n",
    "    cleaned_words = [word for word in words if len(word) > 3 or word in exceptions]\n",
    "    \n",
    "    # Join the cleaned words back into a string\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    return cleaned_text\n",
    "\n",
    "#convert words to numbers\n",
    "def words_to_numbers(text):\n",
    "    word_to_number = {\n",
    "        'one': '1',\n",
    "        'two': '2',\n",
    "        'three': '3',\n",
    "        'four': '4',\n",
    "        'five': '5',\n",
    "        'six': '6',\n",
    "        'seven': '7',\n",
    "        'eight': '8',\n",
    "        'nine': '9',\n",
    "        'ten': '10',\n",
    "        'single': '1'  \n",
    "    }\n",
    "\n",
    "    def replace_word(match):\n",
    "        word = match.group(0).lower()\n",
    "        return word_to_number.get(word, '')\n",
    "\n",
    "    numeric_values = re.findall(r'\\b(?:one|two|three|four|five|six|seven|eight|nine|ten)\\b', str(text), flags=re.IGNORECASE)\n",
    "    extracted_values = [word_to_number[word.lower()] for word in numeric_values] if numeric_values else None\n",
    "    cleaned_text = re.sub(r'\\b(?:one|two|three|four|five|six|seven|eight|nine|ten|single)\\b', replace_word, str(text), flags=re.IGNORECASE)\n",
    "    return (cleaned_text, extracted_values)\n",
    "\n",
    "# extract specified words - use class\n",
    "def extract_classes(text):\n",
    "    # Pattern to match specific words with word breaks at the end and possible word breaks at the start\n",
    "    pattern = r'\\b(?:class\\s\\d+|classes(?:\\s\\d+,)+\\d+)\\b'\n",
    "    \n",
    "    # Find all the matched patterns in the text\n",
    "    specific_words = re.findall(pattern, str(text), flags=re.IGNORECASE)\n",
    "    \n",
    "    # Join the matched patterns into a single string, separated by commas\n",
    "    specific_words_str = ','.join(specific_words) if specific_words else None\n",
    "    return specific_words_str\n",
    "\n",
    "# remove specified words - use class\n",
    "def remove_classes(text):\n",
    "    # Pattern to match specific words with word breaks at the end and possible word breaks at the start\n",
    "    pattern = r'\\b(?:class\\s\\d+|classes(?:\\s\\d+,)+\\d+)\\b'\n",
    "    \n",
    "    # Replace the matched patterns with an empty string\n",
    "    cleaned_text = re.sub(pattern, '', str(text), flags=re.IGNORECASE)\n",
    "    return cleaned_text\n",
    "\n",
    "# extract and remove numeric with copy to new column\n",
    "\n",
    "def extract_and_remove_numeric(text):\n",
    "    def replace_numeric(match):\n",
    "        return ''\n",
    "    \n",
    "    # Pattern to match numbers with word breaks at the start, end, or both\n",
    "    pattern = r'\\b\\d+(?:\\.\\d+)?\\b'\n",
    "    \n",
    "    # Find numeric values\n",
    "    numeric_values = re.findall(pattern, str(text))\n",
    "    extracted_values = ','.join(numeric_values) if numeric_values else None\n",
    "    \n",
    "    # Replace standalone numeric values with an empty string\n",
    "    cleaned_text = re.sub(r'\\b(?:{})\\b'.format('|'.join(numeric_values)), replace_numeric, str(text))\n",
    "    return (cleaned_text, extracted_values)\n",
    "\n",
    "def remove_standalone_numeric(words_list):\n",
    "    pattern = r'\\b\\d+(?:\\.\\d+)?\\b'\n",
    "    return [word for word in words_list if not re.match(pattern, word)]\n",
    "\n",
    "# extract specific words to new column\n",
    "def extract_units(text):\n",
    "    # Pattern to match specific words with word breaks at the end and possible word breaks at the start\n",
    "    pattern = r'\\b(?:hectares|ha|units|dwellinghouse|dwellinghouses|dwellings|metres|m|storey)\\b'\n",
    "    specific_words = re.findall(pattern, str(text), flags=re.IGNORECASE)\n",
    "    return ','.join(specific_words) if specific_words else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f88c8b",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff6dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# add additional optional stop words to the set\n",
    "optional_stop_words = {'and','all','by','for','more','none','not','null','of','or','over','than','with','local','major','class','storey'}\n",
    "stop_words.update(optional_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6266817",
   "metadata": {},
   "source": [
    "### Read in data for Planning Applications from local directory with subset options and drop year 2020 and 2021 due to covid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1555ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in planning app data\n",
    "planapp_gdf_read = gpd.read_file('pub_plnapppol.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0f7f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "planapp_gdf = planapp_gdf_read[['year', 'local_auth', 'proposal','dev_desc','appl_desc','stat_desc', 'xcoord','ycoord']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove 2020 and 2021 covid years\n",
    "planapp_gdf = planapp_gdf[planapp_gdf['year'] != 2020] \n",
    "planapp_gdf = planapp_gdf[planapp_gdf['year'] != 2021]\n",
    "planapp_gdf = planapp_gdf[planapp_gdf['year'] != 3016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b357b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random selection\n",
    "#subset_size = 100000 \n",
    "planapp_gdf = planapp_gdf_read#.sample(n=subset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b38b25",
   "metadata": {},
   "source": [
    "### Define columns for Description, Application, Status, Proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f1a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create columns for later handling\n",
    "planapp_gdf[\"text_proposal\"] = planapp_gdf[\"proposal\"].str.lower()\n",
    "planapp_gdf[\"text_desc\"] = planapp_gdf[\"dev_desc\"].str.lower()\n",
    "planapp_gdf[\"text_app\"] = planapp_gdf[\"appl_desc\"].str.lower()\n",
    "planapp_gdf[\"text_status\"] = planapp_gdf[\"stat_desc\"].str.lower()\n",
    "planapp_gdf[\"feature\"] = planapp_gdf[\"text_proposal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fdca04",
   "metadata": {},
   "source": [
    "### Words to numbers and remove numerics - also download units and quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ecd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change words to numbers\n",
    "planapp_gdf[['feature_cleaned_1', 'numeric_values']] = planapp_gdf['feature'].apply(words_to_numbers).apply(pd.Series)\n",
    "# Extract use classes\n",
    "planapp_gdf['use_class'] = planapp_gdf['feature_cleaned_1'].apply(extract_classes)\n",
    "#planapp_gdf['feature_cleaned_3'] = planapp_gdf['feature_cleaned_2'].apply(remove_specific_words_with_word_break)\n",
    "# Save the DataFrame with the original text and the extracted specific words removed to CSV\n",
    "planapp_gdf['use_class'].to_csv('useclass.csv', index=False)\n",
    "# Apply the function to the \"text_proposal\" column to extract numeric values and remove them\n",
    "planapp_gdf[['feature_cleaned_2', 'quantity']] = planapp_gdf['feature_cleaned_1'].apply(extract_and_remove_numeric).apply(pd.Series)\n",
    "planapp_gdf['feature_cleaned_2'] = planapp_gdf['feature_cleaned_2'].apply(lambda words_list: remove_standalone_numeric(words_list))\n",
    "planapp_gdf['feature_cleaned_2'] = planapp_gdf['feature_cleaned_2'].apply(lambda words_list: ''.join(words_list))\n",
    "#extract and remove units\n",
    "planapp_gdf['units'] = planapp_gdf['feature'].apply(extract_units)\n",
    "planapp_gdf_download = planapp_gdf[['units'],['quantity']]\n",
    "planapp_gdf_download.to_csv('classes_quantities_units2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a351464",
   "metadata": {},
   "source": [
    "### Pull out list of smallest words to identify additional stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b8c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NLTK stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Filter out None values in the 'text_desc' column\n",
    "desc_words = ' '.join(list(planapp_gdf['text_desc'].dropna().values))\n",
    "\n",
    "# Count and find the 3n most frequent words with 4 characters or less\n",
    "word_counter = Counter(desc_words.split())\n",
    "most_frequent = [word for word, count in word_counter.most_common(30) if len(word) <= 4]\n",
    "\n",
    "# Filter the words with 4 characters or less and not present in NLTK stopwords\n",
    "filtered_most_frequent = [word for word in most_frequent if word not in nltk_stopwords]\n",
    "\n",
    "# Filter the words with 4 characters or less from the original word_counter\n",
    "word_counter_filtered = {word: count for word, count in word_counter.items() if len(word) <= 4}\n",
    "\n",
    "# Convert the filtered word_counter dictionary to a DataFrame\n",
    "df_filtered = pd.DataFrame.from_dict(word_counter_filtered, orient='index', columns=[\"count\"])\n",
    "df_filtered.index.name = \"words\"\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df_filtered.to_csv('word_counter_smallest_proposal.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a7fc3",
   "metadata": {},
   "source": [
    "### Pre processing and visualisation via word count and word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f66f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "planapp_gdf[\"feature_cleaned_3\"] = list(map(text_preprocessing, planapp_gdf.feature_cleaned_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b664f1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of lists into a single list of words\n",
    "desc_words_list = [word for sublist in planapp_gdf['feature_cleaned_3'] for word in sublist]\n",
    "\n",
    "# Join all word corpus\n",
    "desc_words = ' '.join(desc_words_list)\n",
    "\n",
    "# Count and find the 30 most frequent after cleaning\n",
    "word_counter = Counter(desc_words.split())\n",
    "most_frequent = word_counter.most_common(20)\n",
    "\n",
    "# Bar plot of frequent words\n",
    "fig = plt.figure(1, figsize=(20, 10))\n",
    "_ = pd.DataFrame(most_frequent, columns=(\"words\", \"count\"))\n",
    "sns.barplot(x='words', y='count', data=_, palette='winter')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c99c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all words from the 'feature_cleaned_3' column into a single list\n",
    "desc_words_list = [word for sublist in planapp_gdf['feature_cleaned_3'] for word in sublist]\n",
    "\n",
    "# Join all words into a single string\n",
    "data = ' '.join(desc_words_list)\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_frequencies = {word: desc_words_list.count(word) for word in set(desc_words_list)}\n",
    "\n",
    "# Generate the word cloud using word frequencies\n",
    "wordcloud = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(word_frequencies)\n",
    "\n",
    "# Display the generated image\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6af43",
   "metadata": {},
   "source": [
    "### Tokenisation of phrases to create words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a635dd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = gensim.corpora.Dictionary(planapp_gdf[\"feature_cleaned_3\"])\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in planapp_gdf[\"feature_cleaned_3\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4a643a",
   "metadata": {},
   "source": [
    "### First model run to calculate coherence and perplexity including bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf72dac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the function to calculate perplexity\n",
    "def calculate_perplexity(lda_model, corpus, dictionary):\n",
    "    return lda_model.log_perplexity(corpus), lda_model.bound(corpus)\n",
    "\n",
    "# Define alpha and beta\n",
    "alpha = 0.5 #'auto'\n",
    "beta = 0.1 #'auto'  \n",
    "\n",
    "texts = planapp_gdf[\"feature_cleaned_3\"]\n",
    "\n",
    "# Create the bigram data\n",
    "bigram = Phrases(texts, min_count=5, threshold=100)\n",
    "bigram_phraser = Phraser(bigram)\n",
    "texts_bigram = [bigram_phraser[text] for text in texts]\n",
    "\n",
    "# Create the trigram data\n",
    "trigram = Phrases(bigram[texts_bigram], min_count=5, threshold=100)\n",
    "trigram_phraser = Phraser(trigram)\n",
    "texts_trigram = [trigram_phraser[bigram_phraser[text]] for text in texts]\n",
    "\n",
    "# Create the dictionary from the trigram data\n",
    "id2word = Dictionary(texts_trigram)\n",
    "\n",
    "# Term Document Frequency (corpus) for the trigram data\n",
    "corpus = [id2word.doc2bow(text) for text in texts_trigram]\n",
    "\n",
    "# Set up the number of topics to iterate over\n",
    "num_topics_range = range(1, 5)  # Choose the range of the number of topics to evaluate\n",
    "\n",
    "# Create lists to store the results\n",
    "number_of_topics = []\n",
    "coherence_scores = []\n",
    "perplexity_scores = []\n",
    "\n",
    "# Train LDA models with different numbers of topics and calculate perplexity and coherence\n",
    "for num_topics in num_topics_range:\n",
    "    lda_model = LdaMulticore(corpus=corpus,\n",
    "                             id2word=id2word,\n",
    "                             iterations=50,\n",
    "                             num_topics=num_topics,\n",
    "                             alpha=alpha,\n",
    "                             eta=beta,  # 'eta' is used for beta\n",
    "                             workers=12,\n",
    "                             passes=10)\n",
    "\n",
    "    perplexity, bound = calculate_perplexity(lda_model, corpus, id2word)\n",
    "    perplexity_scores.append(perplexity)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                         texts=texts_trigram,  # Use the trigram data here\n",
    "                                         dictionary=id2word,\n",
    "                                         coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    number_of_topics.append(num_topics)\n",
    "    coherence_scores.append(coherence_lda)\n",
    "\n",
    "    # Print coherence score and perplexity for each iteration\n",
    "    print(f\"Iteration {num_topics}: Coherence Score = {coherence_lda}, Perplexity = {perplexity}, Bound = {bound}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_metrics = pd.DataFrame({'number_of_topics': number_of_topics,\n",
    "                              'coherence_score': coherence_scores,\n",
    "                              'perplexity_score': perplexity_scores})\n",
    "\n",
    "# Compute moving average with window size 3 (you can adjust the window size as needed)\n",
    "topic_metrics['coherence_score_smoothed'] = topic_metrics['coherence_score'].rolling(window=3, min_periods=1).mean()\n",
    "topic_metrics['perplexity_score_smoothed'] = topic_metrics['perplexity_score'].rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "# Plot the smoothed coherence scores and perplexity scores on the same plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=topic_metrics, x='number_of_topics', y='coherence_score_smoothed', label='Coherence Score')\n",
    "#sns.lineplot(data=topic_metrics, x='number_of_topics', y='perplexity_score', label='Perplexity Score')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Coherence vs. Number of Topics')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebca9267",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "#sns.lineplot(data=topic_metrics, x='number_of_topics', y='coherence_score_smoothed', label='Coherence Score')\n",
    "sns.lineplot(data=topic_metrics, x='number_of_topics', y='perplexity_score_smoothed', label='Perplexity Score')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Perplexity vs. Number of Topics')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b0bf2e",
   "metadata": {},
   "source": [
    "### Second model run with selected number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd09353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of topics \n",
    "n_topics = 13\n",
    "\n",
    "# Run the LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=n_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           alpha=alpha,\n",
    "                                            eta=beta,\n",
    "                                           iterations=50,\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b730caf6",
   "metadata": {},
   "source": [
    "### Check for bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff94dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_list = []\n",
    "# Check if the id2word dictionary contains bigrams and collect them in the bigrams_list\n",
    "for word in id2word.values():\n",
    "    if '_' in word:\n",
    "        bigrams_list.append(word)\n",
    "# Print the bigrams list\n",
    "print(\"List of bigrams:\", bigrams_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5890ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_id in range(num_topics):\n",
    "    topic_words = lda_model.show_topic(topic_id)\n",
    "    print(f\"Topic {topic_id + 1}:\")\n",
    "    for word, probability in topic_words:\n",
    "        print(f\"{word}: {probability:.4f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3986e5b",
   "metadata": {},
   "source": [
    "### Visualisation using pyLDAvis and save to HTML - not of use with bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374f413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Prepare the visualization data\n",
    "vis_data = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "\n",
    "# Convert topic_info to a DataFrame\n",
    "topic_info_df = pd.DataFrame(vis_data.topic_info)\n",
    "\n",
    "# Convert token_table to a DataFrame\n",
    "token_table_df = pd.DataFrame(vis_data.token_table)\n",
    "\n",
    "# Convert topic_coordinates to a DataFrame and replace complex numbers with real values\n",
    "topic_coords_df = pd.DataFrame(vis_data.topic_coordinates.applymap(np.real))\n",
    "\n",
    "# Convert any remaining NaN values to 0\n",
    "topic_coords_df = topic_coords_df.fillna(0)\n",
    "\n",
    "# Create a new PreparedData object with the updated data\n",
    "updated_data = pyLDAvis.PreparedData(topic_coordinates=topic_coords_df,\n",
    "                                    topic_info=topic_info_df,\n",
    "                                    token_table=token_table_df,\n",
    "                                    R=vis_data.R,\n",
    "                                    lambda_step=vis_data.lambda_step,\n",
    "                                    plot_opts=vis_data.plot_opts,\n",
    "                                    topic_order=vis_data.topic_order)\n",
    "\n",
    "# Display the visualization\n",
    "pyLDAvis.display(updated_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad42c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis_data, 'lda_visualization_proposal_tri5aug_v2FINAL.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e04257",
   "metadata": {},
   "source": [
    "### Visualisation of bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eaec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have already created bigrams and trigrams in your text data\n",
    "\n",
    "# Calculate frequencies of bigrams and trigrams in the corpus\n",
    "bigram_frequencies = {}  # Dictionary to store bigram frequencies\n",
    "trigram_frequencies = {}  # Dictionary to store trigram frequencies\n",
    "\n",
    "for text in texts_trigram:  # Assuming texts_trigram contains the trigrams\n",
    "    for word in text:\n",
    "        if '_' in word:  # Check if it is a bigram or trigram\n",
    "            if len(word.split('_')) == 2:  # Bigram\n",
    "                bigram_frequencies[word] = bigram_frequencies.get(word, 0) + 1\n",
    "            elif len(word.split('_')) == 3:  # Trigram\n",
    "                trigram_frequencies[word] = trigram_frequencies.get(word, 0) + 1\n",
    "\n",
    "# Create word clouds for bigrams and trigrams\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac630965",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_wordcloud = WordCloud(background_color='white', width=1000, height=600).generate_from_frequencies(bigram_frequencies)\n",
    "\n",
    "# Plot the word cloud with larger size\n",
    "plt.figure(figsize=(12, 8))  # Set the figure size for the plot\n",
    "plt.imshow(bigram_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "#plt.title('Bigrams Word Cloud', fontsize=20)  # Add a title with a larger font size\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_wordcloud = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(trigram_frequencies)\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Set the figure size for the plot\n",
    "plt.imshow(trigram_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "#plt.title('Trigrams Word Cloud')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
