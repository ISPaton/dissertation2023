{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678e7d39",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b422c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import logging\n",
    "import shapefile\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import re\n",
    "import pyLDAvis.gensim_models\n",
    "import numpy as np\n",
    "import matplotlib.font_manager as fm\n",
    "import gensim\n",
    "import seaborn as sns\n",
    "from os import path\n",
    "from gensim.models import LdaMulticore\n",
    "from PIL import Image\n",
    "from IPython.display import Markdown, display\n",
    "from itertools import chain\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk import pos_tag\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "from collections import Counter\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d21749e",
   "metadata": {},
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f8df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "def text_preprocessing(text):\n",
    "    if text is None:\n",
    "        return []  # Return an empty list if text is None\n",
    "   \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text) \n",
    "\n",
    "    # Tokenize each word\n",
    "    text = nltk.WordPunctTokenizer().tokenize(text)\n",
    "\n",
    "    # Lemmatize each word - Can be disabled using hash\n",
    "    #text = [nltk.stem.WordNetLemmatizer().lemmatize(token, pos='v') for token in text if len(token) > 1]\n",
    "    #text = [nltk.stem.WordNetLemmatizer().lemmatize(token, pos='n') for token in text if len(token) > 1]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    return text\n",
    "\n",
    "# Convert list to string\n",
    "def to_string(text):\n",
    "    text = ' '.join(map(str, text))\n",
    "    return text\n",
    "\n",
    "# clean text\n",
    "def clean_text(text, exceptions=[]):\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    words = text.split()\n",
    "    cleaned_words = [word for word in words if len(word) > 3 or word in exceptions]\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    return cleaned_text\n",
    "\n",
    "#convert words to numbers\n",
    "def words_to_numbers(text):\n",
    "    word_to_number = {\n",
    "        'one': '1',\n",
    "        'two': '2',\n",
    "        'three': '3',\n",
    "        'four': '4',\n",
    "        'five': '5',\n",
    "        'six': '6',\n",
    "        'seven': '7',\n",
    "        'eight': '8',\n",
    "        'nine': '9',\n",
    "        'ten': '10',\n",
    "        'single': '1'  \n",
    "    }\n",
    "\n",
    "    def replace_word(match):\n",
    "        word = match.group(0).lower()\n",
    "        return word_to_number.get(word, '')\n",
    "    numeric_values = re.findall(r'\\b(?:one|two|three|four|five|six|seven|eight|nine|ten)\\b', str(text), flags=re.IGNORECASE)\n",
    "    extracted_values = [word_to_number[word.lower()] for word in numeric_values] if numeric_values else None\n",
    "    cleaned_text = re.sub(r'\\b(?:one|two|three|four|five|six|seven|eight|nine|ten|single)\\b', replace_word, str(text), flags=re.IGNORECASE)\n",
    "    return (cleaned_text, extracted_values)\n",
    "\n",
    "# extract specified words - also for use classes in Proposal field\n",
    "def extract_classes(text):\n",
    "    # Pattern to match specific words with word breaks at the end and possible word breaks at the start\n",
    "    pattern = r'\\b(?:class\\s\\d+|classes(?:\\s\\d+,)+\\d+)\\b'\n",
    "    specific_words = re.findall(pattern, str(text), flags=re.IGNORECASE)\n",
    "    specific_words_str = ','.join(specific_words) if specific_words else None\n",
    "    return specific_words_str\n",
    "\n",
    "# remove specified words - also for use classes in Proposal field\n",
    "def remove_classes(text):\n",
    "    # Pattern to match specific words with word breaks at the end and possible word breaks at the start\n",
    "    pattern = r'\\b(?:class\\s\\d+|classes(?:\\s\\d+,)+\\d+)\\b'\n",
    "    cleaned_text = re.sub(pattern, '', str(text), flags=re.IGNORECASE)\n",
    "    return cleaned_text\n",
    "\n",
    "# extract and remove numeric with copy to new column\n",
    "\n",
    "def extract_and_remove_numeric(text):\n",
    "    def replace_numeric(match):\n",
    "        return ''\n",
    "    pattern = r'\\b\\d+(?:\\.\\d+)?\\b'\n",
    "    numeric_values = re.findall(pattern, str(text))\n",
    "    extracted_values = ','.join(numeric_values) if numeric_values else None\n",
    "    cleaned_text = re.sub(r'\\b(?:{})\\b'.format('|'.join(numeric_values)), replace_numeric, str(text))\n",
    "    return (cleaned_text, extracted_values)\n",
    "\n",
    "def remove_standalone_numeric(words_list):\n",
    "    pattern = r'\\b\\d+(?:\\.\\d+)?\\b'\n",
    "    return [word for word in words_list if not re.match(pattern, word)]\n",
    "\n",
    "# extract specific words to new column\n",
    "def extract_units(text):\n",
    "    pattern = r'\\b(?:hectares|ha|units|dwellinghouse|dwellinghouses|dwellings|metres|m|storey)\\b'\n",
    "    specific_words = re.findall(pattern, str(text), flags=re.IGNORECASE)\n",
    "    return ','.join(specific_words) if specific_words else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d282b52",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fb8196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# add additional optional stop words to the set\n",
    "optional_stop_words = {'and','all','by','for','more','none','not','null','of','or','over','than','with','local','major','minor','maj','sq','m','<','>','1','2','3','4','5','6','7','8','9','ha'}\n",
    "stop_words.update(optional_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b5e264",
   "metadata": {},
   "source": [
    "### Read in data for Planning Applications from local directory with subset options and drop year 2020 and 2021 due to covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1555ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in planning app data\n",
    "planapp_gdf_read = gpd.read_file('pub_plnapppol.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove 2020 and 2021 covid years\n",
    "planapp_gdf = planapp_gdf_read[planapp_gdf_read['year'] != 2020] \n",
    "planapp_gdf = planapp_gdf[planapp_gdf['year'] != 2021]\n",
    "planapp_gdf = planapp_gdf[planapp_gdf['year'] != 3016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b357b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random selection or selection by year or selection by authority - disable via hashtags\n",
    "subset_size = 100000\n",
    "# Define the range of years\n",
    "start_year = 2017\n",
    "end_year = 2019\n",
    "#planapp_gdf_year = planapp_gdf[planapp_gdf['year'].isin(range(start_year, end_year+1))]\n",
    "planapp_gdf_auth = planapp_gdf[planapp_gdf['local_auth'] == 'Glasgow City']\n",
    "#planapp_gdf = planapp_gdf_read.sample(n=subset_size)\n",
    "planapp_gdf = planapp_gdf_auth\n",
    "#planapp_gdf = planapp_gdf_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b38b25",
   "metadata": {},
   "source": [
    "### Define columns for Description, Application, Status, Proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f1a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create columns for later handling\n",
    "planapp_gdf[\"text_proposal\"] = planapp_gdf[\"proposal\"].str.lower()\n",
    "planapp_gdf[\"text_desc\"] = planapp_gdf[\"dev_desc\"].str.lower()\n",
    "planapp_gdf[\"text_app\"] = planapp_gdf[\"appl_desc\"].str.lower()\n",
    "planapp_gdf[\"feature\"] = planapp_gdf[\"text_desc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d3c34e",
   "metadata": {},
   "source": [
    "### Words to numbers and remove numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89818fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the \"text_proposal\" column to extract numeric values and remove them\n",
    "planapp_gdf[['feature_cleaned_1', 'numeric_values']] = planapp_gdf['feature'].apply(words_to_numbers).apply(pd.Series)\n",
    "planapp_gdf[['feature_cleaned_2', 'quantity']] = planapp_gdf['feature_cleaned_1'].apply(extract_and_remove_numeric).apply(pd.Series)\n",
    "planapp_gdf['feature_cleaned_2'] = planapp_gdf['feature_cleaned_2'].apply(lambda words_list: remove_standalone_numeric(words_list))\n",
    "planapp_gdf['feature_cleaned_2'] = planapp_gdf['feature_cleaned_2'].apply(lambda words_list: ''.join(words_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a351464",
   "metadata": {},
   "source": [
    "### Pull out list of smallest words as separate CSV to identify additional stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b8c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out None values in the 'text_desc' column\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "desc_words = ' '.join(list(planapp_gdf['text_desc'].dropna().values))\n",
    "\n",
    "# Count and find the 30 most frequent words with 4 characters or less\n",
    "word_counter = Counter(desc_words.split())\n",
    "most_frequent = [word for word, count in word_counter.most_common(30) if len(word) <= 4]\n",
    "\n",
    "# Filter the words with 4 characters or less and not present in NLTK stopwords\n",
    "filtered_most_frequent = [word for word in most_frequent if word not in stopwords_list]\n",
    "\n",
    "# Filter the words with 4 characters or less from the original word_counter\n",
    "word_counter_filtered = {word: count for word, count in word_counter.items() if len(word) <= 4}\n",
    "\n",
    "# Convert the filtered word_counter dictionary to a DataFrame\n",
    "df_filtered = pd.DataFrame.from_dict(word_counter_filtered, orient='index', columns=[\"count\"])\n",
    "df_filtered.index.name = \"words\"\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df_filtered.to_csv('word_counter_smallest_desc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1fa82e",
   "metadata": {},
   "source": [
    "### Pre processing and visualisation via word count and word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a918fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "planapp_gdf[\"feature_cleaned_3\"] = list(map(text_preprocessing, planapp_gdf.feature_cleaned_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0509c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of lists into a single list of words\n",
    "desc_words_list = [word for sublist in planapp_gdf['feature_cleaned_3'] for word in sublist]\n",
    "\n",
    "# Join all word corpus\n",
    "desc_words = ' '.join(desc_words_list)\n",
    "\n",
    "# Count and find the n most frequent after cleaning\n",
    "word_counter = Counter(desc_words.split())\n",
    "most_frequent = word_counter.most_common(20)\n",
    "\n",
    "# Bar plot of frequent words\n",
    "fig = plt.figure(1, figsize=(20, 10))\n",
    "_ = pd.DataFrame(most_frequent, columns=(\"words\", \"count\"))\n",
    "sns.barplot(x='words', y='count', data=_, palette='winter')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac1baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all words from the 'feature_cleaned_3' column into a single list\n",
    "desc_words_list = [word for sublist in planapp_gdf['feature_cleaned_3'] for word in sublist]\n",
    "\n",
    "# Join all words into a single string\n",
    "data = ' '.join(desc_words_list)\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_frequencies = {word: desc_words_list.count(word) for word in set(desc_words_list)}\n",
    "\n",
    "# Generate the word cloud using word frequencies\n",
    "wordcloud = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(word_frequencies)\n",
    "\n",
    "# Display the generated image\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6af43",
   "metadata": {},
   "source": [
    "### Tokenisation of phrases to create words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b335757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words in each list of the \"feature_cleaned_3\" column\n",
    "planapp_gdf[\"feature_cleaned_3\"] = planapp_gdf[\"feature_cleaned_3\"].apply(lambda x: [word for word in x if isinstance(word, str)])\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = gensim.corpora.Dictionary(planapp_gdf[\"feature_cleaned_3\"])\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in planapp_gdf[\"feature_cleaned_3\"]]\n",
    "\n",
    "# Create a list of lists for text_list\n",
    "text_list = [[text] for text in planapp_gdf[\"feature_cleaned_3\"] if isinstance(text, list) and len(text) > 0]\n",
    "\n",
    "# Flatten the text_list\n",
    "flattened_text_list = list(chain.from_iterable(text_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79144749",
   "metadata": {},
   "source": [
    "### First model run to calculate coherence and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf72dac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Define the function to calculate perplexity\n",
    "def calculate_perplexity(lda_model, corpus, dictionary):\n",
    "    return lda_model.log_perplexity(corpus), lda_model.bound(corpus)\n",
    "\n",
    "# Define alpha and beta\n",
    "alpha = 0.5\n",
    "beta = 0.1  \n",
    "\n",
    "# Set up the number of topics to iterate over\n",
    "num_topics_range = range(1, 31)  # Choose the range of the number of topics to evaluate\n",
    "\n",
    "# Create lists to store the results\n",
    "number_of_topics = []\n",
    "coherence_scores = []\n",
    "perplexity_scores = []\n",
    "\n",
    "# Train LDA models with different numbers of topics and calculate perplexity and coherence\n",
    "for num_topics in num_topics_range:\n",
    "    lda_model = LdaMulticore(corpus=corpus,\n",
    "                             id2word=id2word,\n",
    "                             iterations=50,\n",
    "                             num_topics=num_topics,\n",
    "                             alpha=alpha,\n",
    "                             eta=beta,  # 'eta' is used for beta\n",
    "                             workers=12,\n",
    "                             passes=10)\n",
    "\n",
    "    perplexity, bound = calculate_perplexity(lda_model, corpus, id2word)\n",
    "    perplexity_scores.append(perplexity)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                         texts=flattened_text_list,\n",
    "                                         dictionary=id2word,\n",
    "                                         coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    number_of_topics.append(num_topics)\n",
    "    coherence_scores.append(coherence_lda)\n",
    "\n",
    "    # Print coherence score and perplexity for each iteration\n",
    "    print(f\"Iteration {num_topics}: Coherence Score = {coherence_lda}, Perplexity = {perplexity}, Bound = {bound}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_metrics = pd.DataFrame({'number_of_topics': number_of_topics,\n",
    "                              'coherence_score': coherence_scores,\n",
    "                              'perplexity_score': perplexity_scores})\n",
    "\n",
    "# Compute moving average with window size 3 (you can adjust the window size as needed)\n",
    "topic_metrics['coherence_score_smoothed'] = topic_metrics['coherence_score'].rolling(window=3, min_periods=1).mean()\n",
    "topic_metrics['perplexity_score_smoothed'] = topic_metrics['perplexity_score'].rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "# Plot the smoothed coherence scores and perplexity scores on the same plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=topic_metrics, x='number_of_topics', y='coherence_score_smoothed', label='Coherence Score')\n",
    "#sns.lineplot(data=topic_metrics, x='number_of_topics', y='perplexity_score', label='Perplexity Score')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Coherence vs. Number of Topics')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d67ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "#sns.lineplot(data=topic_metrics, x='number_of_topics', y='coherence_score_smoothed', label='Coherence Score')\n",
    "sns.lineplot(data=topic_metrics, x='number_of_topics', y='perplexity_score_smoothed', label='Perplexity Score')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Perplexity vs. Number of Topics')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5a47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa61497",
   "metadata": {},
   "source": [
    "### Second model run with selected number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc98983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of topics \n",
    "n_topics = 13\n",
    "\n",
    "# Run the LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=n_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           alpha=alpha,\n",
    "                                            eta=beta,\n",
    "                                           iterations=50,\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a274eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for bigrams\n",
    "has_bigrams = any('_' in word for word in id2word.values())\n",
    "\n",
    "if has_bigrams:\n",
    "    print(\"The id2word dictionary contains bigrams.\")\n",
    "else:\n",
    "    print(\"The id2word dictionary does not contain bigrams.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3986e5b",
   "metadata": {},
   "source": [
    "### Visualisation using pyLDAvis and save to HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374f413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Prepare the visualization data\n",
    "vis_data = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "\n",
    "# Convert topic_info to a DataFrame\n",
    "topic_info_df = pd.DataFrame(vis_data.topic_info)\n",
    "\n",
    "# Convert token_table to a DataFrame\n",
    "token_table_df = pd.DataFrame(vis_data.token_table)\n",
    "\n",
    "# Convert topic_coordinates to a DataFrame and replace complex numbers with real values\n",
    "topic_coords_df = pd.DataFrame(vis_data.topic_coordinates.applymap(np.real))\n",
    "\n",
    "# Convert any remaining NaN values to 0\n",
    "topic_coords_df = topic_coords_df.fillna(0)\n",
    "\n",
    "# Create a new PreparedData object with the updated data\n",
    "updated_data = pyLDAvis.PreparedData(topic_coordinates=topic_coords_df,\n",
    "                                    topic_info=topic_info_df,\n",
    "                                    token_table=token_table_df,\n",
    "                                    R=vis_data.R,\n",
    "                                    lambda_step=vis_data.lambda_step,\n",
    "                                    plot_opts=vis_data.plot_opts,\n",
    "                                    topic_order=vis_data.topic_order)\n",
    "\n",
    "# Display the visualization\n",
    "pyLDAvis.display(updated_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ca7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis_data, 'lda_visualization_description_6aug__single_auth_13categories.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
